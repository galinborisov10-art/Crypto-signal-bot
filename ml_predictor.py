"""
ML PREDICTOR - Machine Learning –º–æ–¥–µ–ª –∑–∞ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–∞–Ω–µ –Ω–∞ —É—Å–ø–µ—à–Ω–æ—Å—Ç—Ç–∞ –Ω–∞ —Å–∏–≥–Ω–∞–ª–∏
================================================================================

–¢–æ–∑–∏ –º–æ–¥—É–ª —Ç—Ä–µ–Ω–∏—Ä–∞ Random Forest –º–æ–¥–µ–ª –Ω–∞ –±–∞–∑–∞—Ç–∞ –Ω–∞ trading_journal.json
–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–≤–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—Ç–∞ –∑–∞ —É—Å–ø–µ—Ö –Ω–∞ –Ω–æ–≤–∏ —Ç—Ä–µ–π–¥–æ–≤–µ.

Features –∏–∑–ø–æ–ª–∑–≤–∞–Ω–∏ –∑–∞ ML:
- RSI (14)
- MA(20), MA(50)
- Volume ratio
- Volatility
- BTC correlation
- News sentiment
- Confidence score
- Timeframe

–ê–≤—Ç–æ—Ä: Crypto Signal Bot
–í–µ—Ä—Å–∏—è: 1.0
"""

import json
import os
import logging
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import numpy as np

try:
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import accuracy_score, classification_report
    import joblib
    ML_AVAILABLE = True
except ImportError:
    ML_AVAILABLE = False
    logging.warning("‚ö†Ô∏è scikit-learn –Ω–µ –µ –∏–Ω—Å—Ç–∞–ª–∏—Ä–∞–Ω. ML —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–Ω–æ—Å—Ç –µ –Ω–µ–¥–æ—Å—Ç—ä–ø–Ω–∞.")

logger = logging.getLogger(__name__)


class MLPredictor:
    """Machine Learning –º–æ–¥–µ–ª –∑–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–≤–∞–Ω–µ –Ω–∞ —É—Å–ø–µ—à–Ω–∏ —Ç—Ä–µ–π–¥–æ–≤–µ"""
    
    def __init__(self, model_path='ml_model.pkl', min_training_data=50):
        self.model_path = model_path
        self.min_training_data = min_training_data
        self.model = None
        self.feature_names = [
            'rsi',                      # Keep - RSI indicator
            'market_structure_score',   # NEW - Pure ICT: Market structure (HH/HL vs LH/LL)
            'order_block_strength',     # NEW - Pure ICT: Order block count and quality
            'displacement_score',       # NEW - Pure ICT: Price displacement strength
            'fvg_quality',             # NEW - Pure ICT: Fair Value Gap quality
            'liquidity_grab_score',    # NEW - Pure ICT: Liquidity sweep strength
            'volume_ratio',            # Keep - Volume analysis
            'volatility',              # Keep - Price volatility
            'confidence',              # Keep - ICT confidence score
            'btc_correlation',         # Keep - BTC correlation
            'sentiment_score',         # Keep - Market sentiment
            'mtf_alignment',           # NEW - Pure ICT: Multi-timeframe confluence
            'risk_reward_ratio'        # NEW - Pure ICT: Risk/reward from signal
        ]
        self.is_trained = False
        
        # –ó–∞—Ä–µ–¥–∏ –º–æ–¥–µ–ª –∞–∫–æ —Å—ä—â–µ—Å—Ç–≤—É–≤–∞
        if os.path.exists(model_path):
            try:
                loaded_model = joblib.load(model_path)
                
                # Validate model compatibility (feature count)
                if hasattr(loaded_model, 'n_features_in_'):
                    expected_features = loaded_model.n_features_in_
                    current_features = len(self.feature_names)
                    
                    if expected_features != current_features:
                        logger.warning("=" * 60)
                        logger.warning(f"‚ö†Ô∏è ML MODEL INCOMPATIBILITY DETECTED")
                        logger.warning(f"‚ö†Ô∏è Model expects: {expected_features} features")
                        logger.warning(f"‚ö†Ô∏è Current code has: {current_features} features")
                        logger.warning(f"‚ö†Ô∏è ML Predictor will be DISABLED")
                        logger.warning(f"‚ö†Ô∏è Action: Delete {model_path} and retrain after 50+ trades")
                        logger.warning("=" * 60)
                        self.model = None
                        self.is_trained = False
                    else:
                        self.model = loaded_model
                        self.is_trained = True
                        logger.info(f"‚úÖ ML –º–æ–¥–µ–ª –∑–∞—Ä–µ–¥–µ–Ω –æ—Ç {model_path} ({expected_features} features)")
                else:
                    # Old sklearn version or unsupported model type
                    logger.warning(f"‚ö†Ô∏è Cannot verify feature count for model {model_path}")
                    logger.warning(f"‚ö†Ô∏è Loading anyway, but may cause errors if incompatible")
                    self.model = loaded_model
                    self.is_trained = True
                    
            except Exception as e:
                logger.error(f"‚ùå –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∑–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ ML –º–æ–¥–µ–ª: {e}")
                self.model = None
                self.is_trained = False
    
    def extract_features(self, trade_data: Dict) -> Optional[List[float]]:
        """
        ‚úÖ UPDATED: Extract Pure ICT features (NO MA/EMA!)
        
        Args:
            trade_data: Dictionary with ICT signal data
            
        Returns:
            List of 13 features or None if data is incomplete
            
        Features:
        1. RSI (0-100)
        2. Market Structure Score (0-100) - Pure ICT
        3. Order Block Strength (0-100) - Pure ICT
        4. Displacement Score (0-100) - Pure ICT
        5. FVG Quality (0-100) - Pure ICT
        6. Liquidity Grab Score (0-100) - Pure ICT
        7. Volume Ratio (0-5+)
        8. Volatility (0-10+)
        9. Confidence (0-100)
        10. BTC Correlation (-1 to 1, normalized to 0-100)
        11. Sentiment Score (0-100)
        12. MTF Alignment (0-100)
        13. Risk/Reward Ratio (0-10+)
        """
        try:
            # Try to get ICT components (new format)
            ict_components = trade_data.get('ict_components', {})
            analysis = trade_data.get('analysis_data', {})
            
            # === FEATURE 1: RSI (KEEP AS IS) ===
            rsi = analysis.get('rsi') or trade_data.get('rsi', 50.0)
            if rsi is None or rsi < 0 or rsi > 100:
                rsi = 50.0
            
            # === FEATURE 2: MARKET STRUCTURE SCORE (NEW - Pure ICT) ===
            market_structure = ict_components.get('market_structure', {})
            if isinstance(market_structure, dict):
                # Try to calculate from structure data
                structure_breaks = market_structure.get('structure_breaks', [])
                bos_count = market_structure.get('bos_count', 0)
                choch_count = market_structure.get('choch_count', 0)
                
                # Score based on structure strength
                structure_score = min(100, (bos_count * 20) + (choch_count * 15) + (len(structure_breaks) * 10))
            else:
                structure_score = 50.0  # Neutral default
            
            # === FEATURE 3: ORDER BLOCK STRENGTH (NEW - Pure ICT) ===
            order_blocks = ict_components.get('order_blocks', [])
            if order_blocks and isinstance(order_blocks, list):
                # Calculate strength from order block count and properties
                ob_count = len(order_blocks)
                ob_strength = min(100, ob_count * 20)  # Max 5 OBs = 100%
                
                # Boost if OBs have high strength property
                try:
                    avg_ob_quality = sum(ob.get('strength', 50) for ob in order_blocks if isinstance(ob, dict)) / max(1, ob_count)
                    ob_strength = (ob_strength + avg_ob_quality) / 2
                except:
                    pass
            else:
                ob_strength = 50.0  # Neutral default
            
            # === FEATURE 4: DISPLACEMENT SCORE (NEW - Pure ICT) ===
            displacement = ict_components.get('displacement', {})
            if isinstance(displacement, dict):
                disp_detected = displacement.get('detected', False)
                disp_strength = displacement.get('strength', 50.0)
                disp_score = disp_strength if disp_detected else 50.0
            else:
                disp_score = 50.0  # Neutral default
            
            # === FEATURE 5: FVG QUALITY (NEW - Pure ICT) ===
            fvgs = ict_components.get('fvgs', []) or ict_components.get('fair_value_gaps', [])
            if fvgs and isinstance(fvgs, list):
                fvg_count = len(fvgs)
                fvg_quality = min(100, fvg_count * 25)  # Max 4 FVGs = 100%
                
                # Boost if FVGs have size property
                try:
                    avg_fvg_size = sum(fvg.get('size_percent', 1.0) for fvg in fvgs if isinstance(fvg, dict)) / max(1, fvg_count)
                    fvg_quality = min(100, fvg_quality + (avg_fvg_size * 10))
                except:
                    pass
            else:
                fvg_quality = 50.0  # Neutral default
            
            # === FEATURE 6: LIQUIDITY GRAB SCORE (NEW - Pure ICT) ===
            liquidity_zones = ict_components.get('liquidity_zones', [])
            if liquidity_zones and isinstance(liquidity_zones, list):
                liq_count = len(liquidity_zones)
                liq_score = min(100, liq_count * 15)  # Max 6-7 zones = 100%
            else:
                liq_score = 50.0  # Neutral default
            
            # === FEATURE 7: VOLUME RATIO (KEEP AS IS) ===
            volume_ratio = analysis.get('volume_ratio') or trade_data.get('volume_ratio', 1.0)
            if volume_ratio is None or volume_ratio < 0:
                volume_ratio = 1.0
            
            # === FEATURE 8: VOLATILITY (KEEP AS IS) ===
            volatility = analysis.get('volatility') or trade_data.get('volatility', 1.0)
            if volatility is None or volatility < 0:
                volatility = 1.0
            
            # === FEATURE 9: CONFIDENCE (KEEP AS IS) ===
            confidence = trade_data.get('confidence', 50.0)
            if confidence is None or confidence < 0 or confidence > 100:
                confidence = 50.0
            
            # === FEATURE 10: BTC CORRELATION (KEEP AS IS) ===
            btc_correlation = analysis.get('btc_correlation') or trade_data.get('btc_correlation', 0.0)
            if btc_correlation is None:
                btc_correlation = 0.0
            # Normalize to 0-100 scale: -1 to 1 ‚Üí 0 to 100
            btc_correlation_normalized = (btc_correlation + 1) * 50
            
            # === FEATURE 11: SENTIMENT SCORE (KEEP AS IS) ===
            sentiment_score = analysis.get('sentiment_score') or trade_data.get('sentiment_score', 50.0)
            if sentiment_score is None or sentiment_score < 0 or sentiment_score > 100:
                sentiment_score = 50.0
            
            # === FEATURE 12: MTF ALIGNMENT (NEW - Pure ICT) ===
            mtf_confluence = trade_data.get('mtf_confluence', 0.5)
            if mtf_confluence is None:
                mtf_confluence = 0.5
            mtf_alignment = mtf_confluence * 100  # Convert 0-1 to 0-100
            
            # === FEATURE 13: RISK/REWARD RATIO (NEW - Pure ICT) ===
            risk_reward_ratio = trade_data.get('risk_reward_ratio', 2.0)
            if risk_reward_ratio is None or risk_reward_ratio < 0:
                risk_reward_ratio = 2.0
            
            # ‚úÖ RETURN FEATURE VECTOR (13 features)
            features = [
                float(rsi),
                float(structure_score),
                float(ob_strength),
                float(disp_score),
                float(fvg_quality),
                float(liq_score),
                float(volume_ratio),
                float(volatility),
                float(confidence),
                float(btc_correlation_normalized),
                float(sentiment_score),
                float(mtf_alignment),
                float(risk_reward_ratio)
            ]
            
            # Validate all features are valid numbers
            if any(not isinstance(f, (int, float)) or np.isnan(f) or np.isinf(f) for f in features):
                logger.warning("Invalid feature values detected, using defaults")
                return None
            
            return features
            
        except Exception as e:
            logger.error(f"‚ùå Feature extraction error: {e}")
            return None
    
    def load_training_data(self, journal_path='trading_journal.json') -> Tuple[np.ndarray, np.ndarray]:
        """
        –ó–∞—Ä–µ–∂–¥–∞ training –¥–∞–Ω–Ω–∏ –æ—Ç trading journal
        
        Returns:
            (X, y) - Features –∏ labels
        """
        if not os.path.exists(journal_path):
            logger.warning(f"Trading journal –Ω–µ —Å—ä—â–µ—Å—Ç–≤—É–≤–∞: {journal_path}")
            return np.array([]), np.array([])
        
        try:
            with open(journal_path, 'r', encoding='utf-8') as f:
                journal = json.load(f)
            
            X = []  # Features
            y = []  # Labels (1=SUCCESS, 0=FAILED)
            
            for trade in journal.get('trades', []):
                # –í–∑–µ–º–∏ —Å–∞–º–æ –∑–∞–≤—ä—Ä—à–µ–Ω–∏ —Ç—Ä–µ–π–¥–æ–≤–µ
                if trade.get('outcome') not in ['SUCCESS', 'FAILED']:
                    continue
                
                features = self.extract_features(trade)
                if features is None:
                    continue
                
                X.append(features)
                y.append(1 if trade['outcome'] == 'SUCCESS' else 0)
            
            logger.info(f"üìä –ó–∞—Ä–µ–¥–µ–Ω–∏ {len(X)} —Ç—Ä–µ–π–¥–∞ –∑–∞ –æ–±—É—á–µ–Ω–∏–µ (SUCCESS: {sum(y)}, FAILED: {len(y) - sum(y)})")
            
            return np.array(X), np.array(y)
            
        except Exception as e:
            logger.error(f"‚ùå –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∑–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ training data: {e}")
            return np.array([]), np.array([])
    
    def train(self, retrain=False) -> bool:
        """
        –¢—Ä–µ–Ω–∏—Ä–∞ ML –º–æ–¥–µ–ª–∞ –Ω–∞ –±–∞–∑–∞—Ç–∞ –Ω–∞ trading journal
        
        Args:
            retrain: –ê–∫–æ True, –ø—Ä–µ–ø–æ–∫—Ä–∏–≤–∞ —Å—ä—â–µ—Å—Ç–≤—É–≤–∞—â –º–æ–¥–µ–ª
            
        Returns:
            True –∞–∫–æ –æ–±—É—á–µ–Ω–∏–µ—Ç–æ –µ —É—Å–ø–µ—à–Ω–æ
        """
        if not ML_AVAILABLE:
            logger.error("‚ùå scikit-learn –Ω–µ –µ –Ω–∞–ª–∏—á–µ–Ω. –ò–Ω—Å—Ç–∞–ª–∏—Ä–∞–π —Å: pip install scikit-learn")
            return False
        
        if self.is_trained and not retrain:
            logger.info("‚ÑπÔ∏è ML –º–æ–¥–µ–ª –≤–µ—á–µ –µ —Ç—Ä–µ–Ω–∏—Ä–∞–π. –ò–∑–ø–æ–ª–∑–≤–∞–π retrain=True –∑–∞ –ø—Ä–µ–ø–æ–∫—Ä–∏–≤–∞–Ω–µ.")
            return True
        
        # –ó–∞—Ä–µ–¥–∏ –¥–∞–Ω–Ω–∏
        X, y = self.load_training_data()
        
        # Validate feature consistency
        logger.info(f"üìä Extracted features from {len(X)} trades")
        logger.info(f"üìä Feature dimensions: {len(self.feature_names)} features per trade")
        if len(X) > 0:
            logger.info(f"üìä First trade features: {self.feature_names}")
            logger.info(f"üìä Sample values: {X[0]}")
        
        if len(X) < self.min_training_data:
            logger.warning(f"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç—ä—á–Ω–æ –¥–∞–Ω–Ω–∏ –∑–∞ –æ–±—É—á–µ–Ω–∏–µ. –ù—É–∂–Ω–∏ {self.min_training_data}, –Ω–∞–ª–∏—á–Ω–∏ {len(X)}")
            return False
        
        try:
            # Split –Ω–∞ train/test
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42, stratify=y
            )
            
            # –¢—Ä–µ–Ω–∏—Ä–∞–π Random Forest
            self.model = RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=42,
                class_weight='balanced'  # –ó–∞ –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É SUCCESS/FAILED
            )
            
            logger.info("üîÑ –ó–∞–ø–æ—á–≤–∞–º –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ ML –º–æ–¥–µ–ª...")
            self.model.fit(X_train, y_train)
            
            # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –º–æ–¥–µ–ª–∞
            train_score = self.model.score(X_train, y_train)
            test_score = self.model.score(X_test, y_test)
            
            y_pred = self.model.predict(X_test)
            accuracy = accuracy_score(y_test, y_pred)
            
            logger.info(f"‚úÖ ML –º–æ–¥–µ–ª —Ç—Ä–µ–Ω–∏—Ä–∞–π —É—Å–ø–µ—à–Ω–æ!")
            logger.info(f"üìä Train accuracy: {train_score*100:.1f}%")
            logger.info(f"üìä Test accuracy: {test_score*100:.1f}%")
            logger.info(f"üìä Overall accuracy: {accuracy*100:.1f}%")
            
            # Feature importance
            feature_importance = dict(zip(self.feature_names, self.model.feature_importances_))
            sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)
            
            logger.info("üîç –ù–∞–π-–≤–∞–∂–Ω–∏ features:")
            for fname, importance in sorted_features[:5]:
                logger.info(f"   ‚Ä¢ {fname}: {importance*100:.1f}%")
            
            # –ó–∞–ø–∞–∑–∏ –º–æ–¥–µ–ª–∞
            joblib.dump(self.model, self.model_path)
            logger.info(f"üíæ ML –º–æ–¥–µ–ª –∑–∞–ø–∞–∑–µ–Ω –≤ {self.model_path}")
            
            self.is_trained = True
            return True
            
        except Exception as e:
            logger.error(f"‚ùå –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ ML –º–æ–¥–µ–ª: {e}")
            return False
    
    def predict(self, trade_data: Dict) -> Optional[float]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–≤–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—Ç–∞ –∑–∞ —É—Å–ø–µ—Ö –Ω–∞ –¥–∞–¥–µ–Ω —Ç—Ä–µ–π–¥
        
        Args:
            trade_data: –†–µ—á–Ω–∏–∫ —Å analysis_data
            
        Returns:
            –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç –∑–∞ —É—Å–ø–µ—Ö (0-100%) –∏–ª–∏ None –∞–∫–æ –º–æ–¥–µ–ª –Ω–µ –µ —Ç—Ä–µ–Ω–∏—Ä–∞–π
        """
        if not self.is_trained:
            logger.warning("‚ö†Ô∏è ML –º–æ–¥–µ–ª –Ω–µ –µ —Ç—Ä–µ–Ω–∏—Ä–∞–π. –ò–∑–ø–æ–ª–∑–≤–∞–π train() –ø—ä—Ä–≤–æ.")
            return None
        
        features = self.extract_features(trade_data)
        if features is None:
            return None
        
        try:
            # Predict probability
            features_array = np.array([features])
            probability = self.model.predict_proba(features_array)[0][1]  # Probability of SUCCESS
            
            return probability * 100  # –í—ä—Ä–Ω–∏ –∫–∞—Ç–æ –ø—Ä–æ—Ü–µ–Ω—Ç
            
        except Exception as e:
            logger.error(f"‚ùå –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ ML –ø—Ä–µ–¥–∏–∫—Ü–∏—è: {e}")
            return None
    
    def get_confidence_adjustment(self, ml_probability: float, current_confidence: float) -> float:
        """
        –ò–∑—á–∏—Å–ª—è–≤–∞ –∫–æ—Ä–µ–∫—Ü–∏—è –Ω–∞ confidence –±–∞–∑–∏—Ä–∞–Ω–∞ –Ω–∞ ML –ø—Ä–æ–≥–Ω–æ–∑–∞
        
        Args:
            ml_probability: ML –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç –∑–∞ —É—Å–ø–µ—Ö (0-100)
            current_confidence: –¢–µ–∫—É—â confidence –æ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—è –∞–Ω–∞–ª–∏–∑ (0-100)
            
        Returns:
            –ö–æ—Ä–µ–∫—Ü–∏—è –∑–∞ confidence (-20 –¥–æ +20)
        """
        # –ê–∫–æ ML –µ –º–Ω–æ–≥–æ –ø–æ-—É–≤–µ—Ä–µ–Ω –æ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—è –∞–Ω–∞–ª–∏–∑
        diff = ml_probability - current_confidence
        
        # –û–≥—Ä–∞–Ω–∏—á–∏ –∫–æ—Ä–µ–∫—Ü–∏—è—Ç–∞ –¥–æ ¬±20%
        adjustment = max(-20, min(20, diff * 0.3))
        
        return adjustment


# Singleton instance
_ml_predictor = None

def get_ml_predictor() -> MLPredictor:
    """–í—Ä—ä—â–∞ singleton instance –Ω–∞ ML predictor"""
    global _ml_predictor
    if _ml_predictor is None:
        _ml_predictor = MLPredictor()
    return _ml_predictor


if __name__ == '__main__':
    # Test script
    logging.basicConfig(level=logging.INFO)
    
    predictor = MLPredictor()
    
    # –û–ø–∏—Ç –∑–∞ –æ–±—É—á–µ–Ω–∏–µ
    if predictor.train():
        print("\n‚úÖ ML –º–æ–¥–µ–ª –µ –≥–æ—Ç–æ–≤ –∑–∞ –∏–∑–ø–æ–ª–∑–≤–∞–Ω–µ!")
    else:
        print("\n‚ö†Ô∏è ML –º–æ–¥–µ–ª –Ω–µ –º–æ–∂–µ –¥–∞ –±—ä–¥–µ —Ç—Ä–µ–Ω–∏—Ä–∞–π (–Ω—É–∂–Ω–∏ –ø–æ–Ω–µ 50 –∑–∞–≤—ä—Ä—à–µ–Ω–∏ —Ç—Ä–µ–π–¥–∞)")
